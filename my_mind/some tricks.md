# flash attention(attention 为什么这么慢)

[手撕flash attention](https://zhuanlan.zhihu.com/p/696850636)

[通透理解FlashAttention(含其2代和第3代)：全面降低显存读写、加快计算速度](https://blog.csdn.net/v_JULY_v/article/details/133619540)


# over transformer

[一文通透想颠覆Transformer的Mamba：从SSM、HiPPO、S4到Mamba(被誉为Mamba最佳解读)](https://blog.csdn.net/v_JULY_v/article/details/134923301)

[Awesome-Vision-Mamba-Models](https://github.com/Ruixxxx/Awesome-Vision-Mamba-Models)


# Huggingfacce - TRL PEFT 库

[TRL库](https://hf-mirror.com/docs/trl/main/en/sft_trainer#liger-kernel-increase-20-throughput-and-reduces-60-memory-for-multi-gpu-training)

[trl中的PPO代码解析（炒冷饭版）](https://blog.csdn.net/mch2869253130/article/details/142489298)

[基于trl复现DeepSeek-R1的GRPO训练过程](https://blog.csdn.net/ybdesire/article/details/145716664)

[DPO代码解读-Huggingface的TRL库](https://zhuanlan.zhihu.com/p/696044978)


# pytorch lightning

[Pytorch-Lightning-Template](https://github.com/miracleyoo/pytorch-lightning-template/blob/master/Assets/README_CN.md)

[Pytorch Lightning 完全攻略](https://zhuanlan.zhihu.com/p/353985363)

[pytorch 官方教程](https://lightning.ai/docs/pytorch/stable/starter/introduction.html)
