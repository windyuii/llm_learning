# flash attention(attention 为什么这么慢)

[手撕flash attention](https://zhuanlan.zhihu.com/p/696850636)

[通透理解FlashAttention(含其2代和第3代)：全面降低显存读写、加快计算速度](https://blog.csdn.net/v_JULY_v/article/details/133619540)

[【Transformer 所有模型串讲】Auto-regressive models模型 -XLNet 2](https://zhuanlan.zhihu.com/p/497473224)

# over transformer

[一文通透想颠覆Transformer的Mamba：从SSM、HiPPO、S4到Mamba(被誉为Mamba最佳解读)](https://blog.csdn.net/v_JULY_v/article/details/134923301)

[Awesome-Vision-Mamba-Models](https://github.com/Ruixxxx/Awesome-Vision-Mamba-Models)

# Huggingfacce - TRL PEFT 库

[TRL库](https://hf-mirror.com/docs/trl/main/en/sft_trainer#liger-kernel-increase-20-throughput-and-reduces-60-memory-for-multi-gpu-training)

[trl中的PPO代码解析（炒冷饭版）](https://blog.csdn.net/mch2869253130/article/details/142489298)

[基于trl复现DeepSeek-R1的GRPO训练过程](https://blog.csdn.net/ybdesire/article/details/145716664)

[DPO代码解读-Huggingface的TRL库](https://zhuanlan.zhihu.com/p/696044978)

# pytorch lightning

[Pytorch-Lightning-Template](https://github.com/miracleyoo/pytorch-lightning-template/blob/master/Assets/README_CN.md)

[Pytorch Lightning 完全攻略](https://zhuanlan.zhihu.com/p/353985363)

[pytorch 官方教程](https://lightning.ai/docs/pytorch/stable/starter/introduction.html)

# 数据挖掘

[Python数据分析升级版3期学习笔记](https://tynbl.github.io/docs/python-xxxy-3/)

[【数学建模】相关性分析 - 皮尔逊相关系数 &amp; 斯皮尔曼相关系数](https://naylenv.github.io/2020/08/24/%E3%80%90%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E3%80%91%E7%9B%B8%E5%85%B3%E6%80%A7%E5%88%86%E6%9E%90-%E7%9A%AE%E5%B0%94%E9%80%8A%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0-%E6%96%AF%E7%9A%AE%E5%B0%94%E6%9B%BC%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/)

[欢迎来到凌剑的博客!](https://tynbl.github.io/)

[关联规则挖掘：Apriori算法的深度探讨](https://cloud.tencent.com/developer/article/2348503)


# 可解释性学习

[Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/interpretability.html)
