![alt text](image.png)

estimated time: 4:30 hrs

# Pause Tokens, Infinite Context Window, Bidirectional LLMs

- read the paper [Think before you speak: Training Language Models With Pause Tokens](https://arxiv.org/abs/2310.02226)

- read the paper [Meet in the Middle: A New Pre-training Paradigm](https://arxiv.org/abs/2303.07295)

- watched [Google just Solved the Context Window Challenge for Language Models ?](https://www.youtube.com/watch?v=ANjEFi2lkXQ)

- watched [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](https://www.youtube.com/watch?v=r_UBBfTPcF0)

- read the paper [Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention](https://arxiv.org/abs/2404.07143)

## Notes
![alt text](1.jpg)
![alt text](2.jpg)