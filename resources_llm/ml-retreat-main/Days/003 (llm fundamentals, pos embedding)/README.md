![alt text](image.png)

estimated time: 7:30 hrs 

# LLM fundamentals, Token and Positional Embeddings

- Finished Chapter 1,2 of [Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)

- Read about Mistral's tokenizer 
https://docs.mistral.ai/guides/tokenization/

- comperhensive introduction to the positional encoding [Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)

- ðŸŽ¥ watched 
    - [A Deep Dive: Embeddings, Vectors & Search Algorithms in LLM's](https://www.youtube.com/watch?v=WumStBfoArc)
    ![video screenshot](image-1.png)

    -  [Positional embeddings in transformers EXPLAINED | Demystifying positional encodings.](https://www.youtube.com/watch?v=1biZfFLPRSY)
    ![alt text](image-2.png)

    -   [Positional Encoding in Transformer Neural Networks Explained](https://www.youtube.com/watch?v=ZMxVe-HK174)

## Notes

most of the images in these notes are property of [Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)

![note 1](001_page-0001.jpg)
![note 1](001_page-0002.jpg)
![note 1](001_page-0003.jpg)
![note 1](001_page-0004.jpg)